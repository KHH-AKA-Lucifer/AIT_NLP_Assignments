{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d97b8fa",
   "metadata": {},
   "source": [
    "## A2 Language Modeling \n",
    "\n",
    "### Task 1: Dataset Acquisition \n",
    "\n",
    "I'm a big StarWar fun. I searched through the Internet and found this dataset on Github that has all character background and the story line in a single source. One little problem is they are all in html file. Therefore, I need to remove the html tags from the file and extract the text only.\n",
    "\n",
    "Data Source: https://github.com/AlbertoFormaggio1/star_wars_unstructured_dataset/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157d6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries \n",
    "import re\n",
    "from pathlib import Path \n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31217999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path \n",
    "dataset_dir = Path(\"starwars_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3ed578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2txt(html:str) -> str:\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # remove html tags\n",
    "    for tag in soup(['script', 'style', 'noscript', 'header', 'footer', 'nav', 'aside']):\n",
    "        tag.decompose()\n",
    "\n",
    "    \n",
    "    # get the text \n",
    "    text = soup.get_text(separator=' ')\n",
    "\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5adf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty list \n",
    "docs = []\n",
    "meta_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc614943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of HTML files: 242\n"
     ]
    }
   ],
   "source": [
    "html_files = sorted(dataset_dir.glob('*.html'))\n",
    "print(\"# of HTML files:\", len(html_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96abe876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 242/242 [00:00<00:00, 1621.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable documents: 151\n",
      "Sample text: Abafar A desert planet located in the Outer Rim with a completely white surface. Known as The Void, the planet is barely populated but is home to massive amounts of rhydonium, a scarce and volatile fuel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for fp in tqdm(html_files):\n",
    "    html = fp.read_text(encoding='utf-8', errors='ignore')\n",
    "    text = html2txt(html)\n",
    "\n",
    "    if len(text) < 200:\n",
    "        continue \n",
    "\n",
    "    docs.append(text)\n",
    "    meta_data.append(fp.name)\n",
    "\n",
    "\n",
    "# no. of usable documents\n",
    "print('Usable documents:', len(docs))\n",
    "# sample text \n",
    "print('Sample text:', docs[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fa7387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: /Users/kaungheinhtet/Desktop/AIT_NLP_Assignments/A2_Language_Modeling/starwars_dataset/starwars_corpus.txt\n",
      "Total characters: 614127\n"
     ]
    }
   ],
   "source": [
    "# form corpus \n",
    "corpus = '\\n\\n'.join(docs)\n",
    "# define output path \n",
    "out_path = Path('starwars_dataset/starwars_corpus.txt')\n",
    "out_path.write_text(corpus, encoding='utf-8')\n",
    "\n",
    "print('Output:', out_path.resolve())\n",
    "print('Total characters:', len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724706ae",
   "metadata": {},
   "source": [
    "### Preprocesing\n",
    "\n",
    "We have the corpus now. However, the computer does not understand our ligustic words. It only understands the number like 0 and 1. Therfore, we need to tokenize and numericalize as usual to feed the corupus into our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e14c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaungheinhtet/Desktop/AIT_NLP_Assignments/.venv/lib/python3.9/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/kaungheinhtet/Desktop/AIT_NLP_Assignments/.venv/lib/python3.9/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/kaungheinhtet/Desktop/AIT_NLP_Assignments/.venv/lib/python3.9/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# import required libraries \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "279dfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -c \"import torch; print('torch', torch.__version__)\"\n",
    "# !python -c \"import importlib.metadata as m; print('torchtext', m.version('torchtext'))\"\n",
    "# !python -c \"import platform; print(platform.platform()); import struct; print('py', struct.calcsize('P')*8, 'bit')\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb95c5",
   "metadata": {},
   "source": [
    "Let's check if we are using GPU or CPU. And also set the random seed to make sure the initialization are reproducible in all environments and devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6075a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6ad07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars: 614127\n",
      "Abafar A desert planet located in the Outer Rim with a completely white surface. Known as The Void, the planet is barely populated but is home to massive amounts of rhydonium, a scarce and volatile fuel.\n",
      "\n",
      "Admiral Ackbar Fleet Admiral Gial Ackbar is a fictional character from the Star Wars franchise.\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "corpus = Path(\"starwars_dataset/starwars_corpus.txt\")\n",
    "text = corpus.read_text(encoding=\"utf-8\")\n",
    "print(\"Chars:\", len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c77426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use basic english tokenizer\n",
    "# splits + lowercase + basic cleaning\n",
    "tokenizer = get_tokenizer(\"basic_english\")  \n",
    "\n",
    "def yield_tokens(text):\n",
    "    # tokenize by lines to avoid huge memory spikes\n",
    "    for line in text.splitlines():\n",
    "        toks = tokenizer(line)\n",
    "        if toks:\n",
    "            yield toks\n",
    "\n",
    "specials = [\"<pad>\", \"<unk>\"]\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text), specials=specials, min_freq=1)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f993d31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9995\n",
      "Sample tokens: ['darth', 'vader', 'is', 'coming', 'to', 'tatooine', '.']\n",
      "Sample ids: [81, 70, 10, 2004, 6, 212, 4]\n"
     ]
    }
   ],
   "source": [
    "# print no. of vocab size\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "# sample tokens and token ids\n",
    "print(\"Sample tokens:\", tokenizer(\"Darth Vader is coming to Tatooine.\"))\n",
    "print(\"Sample ids:\", vocab(tokenizer(\"Darth Vader is coming to Tatooine.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d9f5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 117284\n",
      "First 30 ids: [3234, 8, 640, 53, 969, 9, 2, 619, 509, 15, 8, 2005, 1719, 1056, 4, 180, 14, 2, 9835, 3, 2, 53, 10, 1421, 3755, 33, 10, 294, 6, 2114]\n"
     ]
    }
   ],
   "source": [
    "# numericalization\n",
    "token_ids = []\n",
    "for line in text.splitlines():\n",
    "    toks = tokenizer(line)\n",
    "    if toks:\n",
    "        token_ids.extend(vocab(toks))\n",
    "\n",
    "print(\"Total tokens:\", len(token_ids))\n",
    "print(\"First 30 ids:\", token_ids[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "905a2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([117254, 30]) Y shape: torch.Size([117254, 30])\n"
     ]
    }
   ],
   "source": [
    "# create LM sequences\n",
    "def make_lm_data(token_ids, seq_len=30):\n",
    "    xs, ys = [], []\n",
    "    for i in range(0, len(token_ids) - seq_len):\n",
    "        chunk = token_ids[i:i+seq_len+1]\n",
    "        xs.append(chunk[:-1])\n",
    "        ys.append(chunk[1:])\n",
    "    return torch.tensor(xs, dtype=torch.long), torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "SEQ_LEN = 30\n",
    "X, Y = make_lm_data(token_ids, seq_len=SEQ_LEN)\n",
    "print(\"X shape:\", X.shape, \"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "844a159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b0741",
   "metadata": {},
   "source": [
    "### Task 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19507189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=256, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)                 # [B, T] -> [B, T, E]\n",
    "        out, hidden = self.lstm(x, hidden)    # [B, T, H]\n",
    "        logits = self.fc(out)                 # [B, T, V]\n",
    "        return logits, hidden\n",
    "\n",
    "model = LSTMLM(vocab_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17ceee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=5.7041 val_loss=4.7895\n",
      "Epoch 2: train_loss=4.2693 val_loss=3.6506\n",
      "Epoch 3: train_loss=3.2072 val_loss=2.5842\n",
      "Epoch 4: train_loss=2.3262 val_loss=1.8200\n",
      "Epoch 5: train_loss=1.7348 val_loss=1.3299\n",
      "Epoch 6: train_loss=1.3491 val_loss=1.0073\n",
      "Epoch 7: train_loss=1.0855 val_loss=0.7939\n",
      "Epoch 8: train_loss=0.9012 val_loss=0.6572\n",
      "Epoch 9: train_loss=0.7706 val_loss=0.5637\n",
      "Epoch 10: train_loss=0.6761 val_loss=0.5032\n",
      "Epoch 11: train_loss=0.6087 val_loss=0.4676\n",
      "Epoch 12: train_loss=0.5579 val_loss=0.4417\n",
      "Epoch 13: train_loss=0.5203 val_loss=0.4237\n",
      "Epoch 14: train_loss=0.4896 val_loss=0.4099\n",
      "Epoch 15: train_loss=0.4655 val_loss=0.4004\n",
      "Epoch 16: train_loss=0.4457 val_loss=0.3913\n",
      "Epoch 17: train_loss=0.4292 val_loss=0.3857\n",
      "Epoch 18: train_loss=0.4150 val_loss=0.3812\n",
      "Epoch 19: train_loss=0.4025 val_loss=0.3774\n",
      "Epoch 20: train_loss=0.3919 val_loss=0.3727\n",
      "Epoch 21: train_loss=0.3823 val_loss=0.3693\n",
      "Epoch 22: train_loss=0.3742 val_loss=0.3670\n",
      "Epoch 23: train_loss=0.3662 val_loss=0.3630\n",
      "Epoch 24: train_loss=0.3592 val_loss=0.3597\n",
      "Epoch 25: train_loss=0.3531 val_loss=0.3578\n",
      "Epoch 26: train_loss=0.3477 val_loss=0.3566\n",
      "Epoch 27: train_loss=0.3424 val_loss=0.3556\n",
      "Epoch 28: train_loss=0.3371 val_loss=0.3541\n",
      "Epoch 29: train_loss=0.3331 val_loss=0.3529\n",
      "Epoch 30: train_loss=0.3289 val_loss=0.3517\n",
      "Epoch 31: train_loss=0.3254 val_loss=0.3498\n",
      "Epoch 32: train_loss=0.3213 val_loss=0.3485\n",
      "Epoch 33: train_loss=0.3180 val_loss=0.3482\n",
      "Epoch 34: train_loss=0.3149 val_loss=0.3468\n",
      "Epoch 35: train_loss=0.3123 val_loss=0.3480\n",
      "Epoch 36: train_loss=0.3093 val_loss=0.3460\n",
      "Epoch 37: train_loss=0.3068 val_loss=0.3460\n",
      "Epoch 38: train_loss=0.3043 val_loss=0.3451\n",
      "Epoch 39: train_loss=0.3021 val_loss=0.3444\n",
      "Epoch 40: train_loss=0.2999 val_loss=0.3437\n",
      "Epoch 41: train_loss=0.2975 val_loss=0.3441\n",
      "Epoch 42: train_loss=0.2956 val_loss=0.3441\n",
      "Epoch 43: train_loss=0.2939 val_loss=0.3433\n",
      "Epoch 44: train_loss=0.2921 val_loss=0.3422\n",
      "Epoch 45: train_loss=0.2904 val_loss=0.3411\n",
      "Epoch 46: train_loss=0.2889 val_loss=0.3418\n",
      "Epoch 47: train_loss=0.2874 val_loss=0.3407\n",
      "Epoch 48: train_loss=0.2856 val_loss=0.3408\n",
      "Epoch 49: train_loss=0.2840 val_loss=0.3409\n",
      "Epoch 50: train_loss=0.2829 val_loss=0.3399\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # targets are token ids\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def run_epoch(model, loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits, _ = model(x)  # [B, T, V]\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "EPOCHS = 50\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = run_epoch(model, train_loader, train=True)\n",
    "    val_loss = run_epoch(model, val_loader, train=False)\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f79218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darth vader freezes solo in carbonite , bounty hunter boba fett delivers him to jabba , who puts solo on display in his palace . later , princess leia comes to save han but is captured by jabba , who turns her\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#import random\n",
    "\n",
    "def generate(model, prompt, max_new_tokens=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    toks = tokenizer(prompt)\n",
    "    ids = vocab(toks)\n",
    "    x = torch.tensor([ids], dtype=torch.long).to(device)\n",
    "\n",
    "    hidden = None\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, hidden = model(x, hidden)\n",
    "        next_logits = logits[0, -1, :] / max(temperature, 1e-8)\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        ids.append(next_id)\n",
    "        x = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "\n",
    "    # decode (simple: join tokens)\n",
    "    inv_vocab = vocab.get_itos()\n",
    "    return \" \".join(inv_vocab[i] for i in ids)\n",
    "\n",
    "print(generate(model, \"darth vader\", max_new_tokens=40, temperature=0.9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8837a05",
   "metadata": {},
   "source": [
    "After training, the LSTM language model parameters were saved using PyTorch’s state dictionary mechanism. The vocabulary mappings, token-to-index and index-to-token, were serialized separately in JSON format to ensure reproducibility and compatibility with the deployment environment. These artifacts will be later reused in the web application for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e56c625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artefacts folder \n",
    "\n",
    "artefact_dir = Path('artefacts')\n",
    "artefact_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d9069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: /Users/kaungheinhtet/Desktop/AIT_NLP_Assignments/A2_Language_Modeling/artefacts/starwars_lstm_model.pt\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model_path = artefact_dir / 'starwars_lstm_model.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(\"Model saved to:\", model_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23773ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab saved\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# idex -> token \n",
    "itos = vocab.get_itos()\n",
    "\n",
    "# token -> index \n",
    "stoi = {token: idx for idx, token in enumerate(itos)}\n",
    "\n",
    "with open(artefact_dir / 'vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            'itos': itos,\n",
    "            'stoi': stoi\n",
    "        }, f, ensure_ascii=False, indent=2\n",
    "    )\n",
    "\n",
    "print('Vocab saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497d593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
