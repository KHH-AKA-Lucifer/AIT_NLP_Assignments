{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee2e286",
   "metadata": {},
   "source": [
    "### Sentence Embedding with Sentence BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17cb7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaungheinhtet/Desktop/AIT_NLP_Assignments/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load the trained model from task 1\n",
    "import re\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b1b206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5887755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bert_mlm.pt\n",
      "MAX_LEN: 1000 H: 256 vocab_size: 20889\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint (vocab + config)\n",
    "pretrained_model = torch.load(\"artefacts/bert_mlm.pt\", map_location=\"cpu\")\n",
    "config = pretrained_model[\"config\"]\n",
    "word2id = pretrained_model[\"word2id\"]\n",
    "id2word = pretrained_model[\"id2word\"]\n",
    "\n",
    "PAD_ID = word2id[\"[PAD]\"]\n",
    "UNK_ID = word2id[\"[UNK]\"]\n",
    "MAX_LEN = config[\"max_len\"]\n",
    "H = config[\"d_model\"]\n",
    "\n",
    "print(\"Loaded bert_mlm.pt\")\n",
    "print(\"MAX_LEN:\", MAX_LEN, \"H:\", H, \"vocab_size:\", config[\"vocab_size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe1c873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bert_encoder.pt\n"
     ]
    }
   ],
   "source": [
    "encoder_path = \"artefacts/bert_encoder.pt\"\n",
    "\n",
    "encoder = torch.jit.load(encoder_path, map_location=\"cpu\")\n",
    "encoder.eval()\n",
    "\n",
    "print(\"Loaded bert_encoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300fa372",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c5cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the same as task 1 \n",
    "def clean_text(s: str) -> str:\n",
    "    # lower case the character\n",
    "    s = s.lower()\n",
    "    # remove punctuation\n",
    "    s = re.sub(r\"[.,!\\-]\", \"\", s)   \n",
    "    # remove white spaces \n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def encode_sentence(sentence: str, max_len: int):\n",
    "    \"\"\"\n",
    "    sentence -> (input_ids, attention_mask)\n",
    "    input_ids: padded to max_len\n",
    "    attention_mask: 1 for real tokens, 0 for PAD\n",
    "    \"\"\"\n",
    "    sentence = clean_text(sentence)\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    ids = [word2id.get(w, UNK_ID) for w in tokens][:max_len]\n",
    "    attn = [1] * len(ids)\n",
    "\n",
    "    while len(ids) < max_len:\n",
    "        ids.append(PAD_ID)\n",
    "        attn.append(0)  \n",
    "\n",
    "    return ids, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e819719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real tokens: 6 UNK tokens: 0\n",
      "UNK ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# quick check on the corpus\n",
    "test_sent = \"A man is playing basketball outdoors.\"\n",
    "ids, attn = encode_sentence(test_sent, MAX_LEN)\n",
    "\n",
    "unk_count = sum(1 for i in ids if i == UNK_ID)\n",
    "real_tokens = sum(attn)\n",
    "\n",
    "print(\"Real tokens:\", real_tokens, \"UNK tokens:\", unk_count)\n",
    "print(\"UNK ratio:\", unk_count / max(real_tokens, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c29377b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNLI: 20000 3000\n"
     ]
    }
   ],
   "source": [
    "snli = load_dataset(\"snli\")\n",
    "snli = snli.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "# Start smaller for speed; increase later\n",
    "train_ds = snli[\"train\"].shuffle(seed=42).select(range(20000))\n",
    "val_ds   = snli[\"validation\"].shuffle(seed=42).select(range(3000))\n",
    "\n",
    "print(\"SNLI:\", len(train_ds), len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe032a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    prem_ids, prem_attn = [], []\n",
    "    hyp_ids, hyp_attn = [], []\n",
    "    labels = []\n",
    "\n",
    "    for x in batch:\n",
    "        p_ids, p_att = encode_sentence(x[\"premise\"], MAX_LEN)\n",
    "        h_ids, h_att = encode_sentence(x[\"hypothesis\"], MAX_LEN)\n",
    "\n",
    "        prem_ids.append(p_ids); prem_attn.append(p_att)\n",
    "        hyp_ids.append(h_ids);  hyp_attn.append(h_att)\n",
    "        labels.append(x[\"label\"])  # 0 entailment, 1 neutral, 2 contradiction\n",
    "\n",
    "    return (\n",
    "        torch.tensor(prem_ids, dtype=torch.long),\n",
    "        torch.tensor(prem_attn, dtype=torch.long),\n",
    "        torch.tensor(hyp_ids, dtype=torch.long),\n",
    "        torch.tensor(hyp_attn, dtype=torch.long),\n",
    "        torch.tensor(labels, dtype=torch.long),\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b944131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    # token_embeddings: [B,S,H]\n",
    "    # attention_mask: [B,S]\n",
    "    mask = attention_mask.unsqueeze(-1).float()         # [B,S,1]\n",
    "    summed = (token_embeddings * mask).sum(dim=1)       # [B,H]\n",
    "    count = mask.sum(dim=1).clamp(min=1e-9)             # [B,1]\n",
    "    return summed / count                               # [B,H]\n",
    "\n",
    "class SBERTSoftmax(nn.Module):\n",
    "    \"\"\"\n",
    "    Features = [u, v, |u-v|], then softmax classifier for NLI (3 classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, hidden_size):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(hidden_size * 3, 3)\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        segment_ids = torch.zeros_like(input_ids)  # [B,S]\n",
    "        hidden = self.encoder(input_ids, segment_ids)         # [B,S,H]\n",
    "        sent_emb = mean_pooling(hidden, attention_mask)       # [B,H]\n",
    "        return sent_emb\n",
    "\n",
    "    def forward(self, prem_ids, prem_attn, hyp_ids, hyp_attn):\n",
    "        u = self.encode(prem_ids, prem_attn)\n",
    "        v = self.encode(hyp_ids, hyp_attn)\n",
    "        feats = torch.cat([u, v, torch.abs(u - v)], dim=1)\n",
    "        logits = self.classifier(feats)\n",
    "        return logits\n",
    "\n",
    "sbert = SBERTSoftmax(encoder, H).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(sbert.parameters(), lr=2e-5)  # paper-style LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32ef204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [2:20:45<00:00,  6.76s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 1.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for ep in range(epochs):\n",
    "    sbert.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for prem_ids, prem_attn, hyp_ids, hyp_attn, labels in tqdm(train_loader):\n",
    "        prem_ids = prem_ids.to(device)\n",
    "        prem_attn = prem_attn.to(device)\n",
    "        hyp_ids = hyp_ids.to(device)\n",
    "        hyp_attn = hyp_attn.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = sbert(prem_ids, prem_attn, hyp_ids, hyp_attn)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {ep+1} avg loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59338f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sbert-softmax_snli model is saved\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"sbert_state_dict\": sbert.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"word2id\": word2id,\n",
    "    \"id2word\": id2word,\n",
    "    },\n",
    "    \"artefacts/sbert_softmax_snli.pt\")\n",
    "\n",
    "print(\"Sbert-softmax_snli model is saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
